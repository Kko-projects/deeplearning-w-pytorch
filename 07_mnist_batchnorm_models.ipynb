{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzpbGDY2Tt5ujJJgNNcZvC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjB4D-FSUrxN"
      },
      "outputs": [],
      "source": [
        "# MNIST classification with Batch Normalization and various model architectures\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "# Define transformation with normalization to scale pixel values between -1 and 1\n",
        "transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load MNIST dataset\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)  # Training set\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # Test set\n",
        "\n",
        "# Create data loaders to feed data in batches\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)  # Shuffle for training\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)   # No shuffle for evaluation\n",
        "\n",
        "# Define Trainer class to handle training and testing\n",
        "class Trainer():\n",
        "    def __init__(self, trainloader, testloader, net, optimizer, criterion):\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "        self.net = net\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def train(self, epoch=100):  # Train for given number of epochs\n",
        "        self.net.train()\n",
        "        for e in range(epoch):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.trainloader, 0):\n",
        "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
        "                self.optimizer.zero_grad()           # Clear previous gradients\n",
        "                output = self.net(inputs)            # Forward pass\n",
        "                loss = self.criterion(output, labels)  # Compute loss\n",
        "                loss.backward()                      # Backpropagation\n",
        "                self.optimizer.step()                # Update parameters\n",
        "                running_loss += loss.item()\n",
        "                if i % 500 == 0:\n",
        "                    print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n",
        "                    running_loss = 0.0\n",
        "                    self.test()  # Optionally evaluate during training\n",
        "        print('Finished Training')\n",
        "\n",
        "    def test(self):  # Evaluate on test data\n",
        "        self.net.eval()\n",
        "        correct = 0\n",
        "        for inputs, labels in self.testloader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            output = self.net(inputs)\n",
        "            pred = output.max(1, keepdim=True)[1]  # Predicted label\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            correct, len(self.testloader.dataset), 100. * correct / len(self.testloader.dataset)))\n",
        "\n",
        "\n",
        "\n",
        "# --------- Model 1: 2-Layer Fully Connected Network with BatchNorm ---------\n",
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        self.fc0 = nn.Linear(28*28, 30)      # Fully connected layer from input to hidden\n",
        "        self.bn0 = nn.BatchNorm1d(30)        # Batch normalization on hidden layer\n",
        "        self.fc1 = nn.Linear(30, 10)         # Output layer for 10 classes\n",
        "        self.act = nn.ReLU()                 # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)                # Flatten image\n",
        "        x = self.fc0(x)\n",
        "        x = self.bn0(x)                      # Apply batch normalization\n",
        "        x = self.act(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "mnist_net = MNIST_Net().cuda()\n",
        "criterion = nn.CrossEntropyLoss()            # Loss function for classification\n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "trainer = Trainer(trainloader=trainloader,\n",
        "                  testloader=testloader,\n",
        "                  net=mnist_net,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer)\n",
        "\n",
        "trainer.train(epoch=4)\n",
        "trainer.test()\n",
        "\n",
        "# Count the number of trainable parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(mnist_net)\n",
        "\n",
        "\n",
        "\n",
        "# --------- Model 2: Conv + Fully Connected + BatchNorm ---------\n",
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=6, stride=2)  # Convolutional layer\n",
        "        self.conv0_bn = nn.BatchNorm2d(8)      # Batch norm after conv\n",
        "        self.fc0 = nn.Linear(8*12*12, 10)      # Fully connected to output\n",
        "        self.act = nn.ReLU()                   # Activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.conv0_bn(x)\n",
        "        x = self.act(x)\n",
        "        x = x.view(x.shape[0], -1)             # Flatten\n",
        "        x = self.fc0(x)\n",
        "        return x\n",
        "\n",
        "mnist_net = MNIST_Net().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)\n",
        "\n",
        "trainer = Trainer(trainloader=trainloader,\n",
        "                  testloader=testloader,\n",
        "                  net=mnist_net,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer)\n",
        "\n",
        "trainer.train(epoch=4)\n",
        "trainer.test()\n",
        "count_parameters(mnist_net)\n",
        "\n",
        "# --------- Model 3: Conv + Pool + Fully Connected + BatchNorm ---------\n",
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=6, stride=2)  # Conv layer\n",
        "        self.pool0 = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling\n",
        "        self.conv0_bn = nn.BatchNorm2d(8)      # Batch norm\n",
        "        self.fc0 = nn.Linear(8*6*6, 10)         # Fully connected output\n",
        "        self.act = nn.ReLU()                   # Activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.pool0(x)                      # Apply pooling\n",
        "        x = self.conv0_bn(x)                   # Apply batch norm\n",
        "        x = self.act(x)\n",
        "        x = x.view(x.shape[0], -1)             # Flatten\n",
        "        x = self.fc0(x)\n",
        "        return x\n",
        "\n",
        "mnist_net = MNIST_Net().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)\n",
        "\n",
        "trainer = Trainer(trainloader=trainloader,\n",
        "                  testloader=testloader,\n",
        "                  net=mnist_net,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer)\n",
        "\n",
        "trainer.train(epoch=4)\n",
        "trainer.test()"
      ]
    }
  ]
}