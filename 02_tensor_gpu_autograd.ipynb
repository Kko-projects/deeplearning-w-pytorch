{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwE39O6qjGWAgB5+KZMSsk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc9VnbG1SSE0"
      },
      "outputs": [],
      "source": [
        "# 02_tensor_gpu_autograd.ipynb\n",
        "# GPU usage and autograd (computational graph, gradients)\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# Move tensors to GPU and back\n",
        "x = torch.tensor([1, 1]).cuda()\n",
        "print(x)\n",
        "x = x.cpu()\n",
        "print(x)\n",
        "\n",
        "x = torch.tensor([1.]).cuda()\n",
        "y = torch.tensor([0.5, 0.1]).cuda()\n",
        "print(x.device, y.device)\n",
        "print(x + y)\n",
        "\n",
        "# Computational graph example\n",
        "a = torch.tensor([2.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)\n",
        "\n",
        "c = a + b\n",
        "d = b + 1\n",
        "e = c + d\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)\n",
        "print(e)\n",
        "\n",
        "print(a.data)\n",
        "print(a.grad)  # None before backward\n",
        "\n",
        "# Retain gradients and backward\n",
        "c.retain_grad()\n",
        "d.retain_grad()\n",
        "e.retain_grad()\n",
        "\n",
        "e.backward()\n",
        "\n",
        "print(a.grad)\n",
        "print(b.grad)\n",
        "print(c.grad)\n",
        "print(d.grad)\n",
        "print(e.grad)\n",
        "\n",
        "# Zero gradients\n",
        "a.grad.zero_()\n",
        "b.grad.zero_()\n",
        "c.grad.zero_()\n",
        "d.grad.zero_()\n",
        "e.grad.zero_()\n",
        "\n",
        "print(a.grad)\n",
        "print(b.grad)\n",
        "print(c.grad)\n",
        "print(d.grad)\n",
        "print(e.grad)\n",
        "\n",
        "# Disable gradient tracking\n",
        "a.requires_grad = False\n",
        "print(a.requires_grad)"
      ]
    }
  ]
}